services:
  llama-stack-server:
    build:
      context: .
      dockerfile: Containerfile
    image: llamastack-m
    # command: llama stack run /root/.llama/distributions/llamastack-m/llamastack-m-run.yaml
    ports:
      - "8321:8321"
    environment:
      - VLLM_URL=$VLLM_URL
      - VLLM_API_TOKEN=$VLLM_API_TOKEN
      - LLAMA_STACK_LOGGING=all=debug
      - TAVILY_API_KEY=$TAVILY_API_KEY
    # volumes:
    #   - ./llamastack-m:/root/.llama/distributions/llamastack-m/